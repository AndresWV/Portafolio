img,title,resume,description,projectLink,technologies,complejidad
icoProjects.jpg,"Pipeline - ""El Quijote de la Mancha""","A pipeline is created to process data from an input file in TXT format (Quixote for this example) and then basic operations are performed, generating an output file.","This Python code implements a pipeline using Apache Beam to process data from a text file. The goal is to count the frequency of each word in the file and generate a list of the most frequent words, ordered by their number of occurrences. The process is detailed below:

    Reading Arguments: Input arguments are defined to specify the input file, the output file and the number of most frequent words to include in the result.

    Definition of Cleaning Functions:
        remove_characters: Removes unwanted characters such as commas, periods, colons, hyphens, and spaces.
        remove_tilde: Converts accented letters to their unaccented version and converts all letters to lowercase.
        clean_words: Apply both previous functions to clean each word.

    Apache Beam Pipeline:
        Reading: The input text file is read, generating a collection of lines.
        Separation: Each line is divided into individual words.
        Cleaning: Each word is cleaned of unwanted characters and accents.
        Count: The occurrences of each word are counted.
        Sorting: The most frequent words are selected according to the number specified in the arguments.
        CSV format: Words and their counts are formatted as CSV (word, count) strings.
        Writing: The formatted result is written to the specified output file.

The pipeline runs using Apache Beam, and can be configured to run in different environments (for example, on-premises or in the cloud) using PipelineOptions. This code is useful for analyzing long texts, such as ""Don Quixote of La Mancha"", to find and list the most common words.",https://github.com/AndresWV/00-Pipeline,"PYTHON, APACHE BEAM",5
icoProjects.jpg,Exploratory Analysis and Data Manipulation of the Iris Dataset with dplyr,"This code uses the dplyr package in R to perform various data manipulation and exploration operations on the iris dataset provided by the datasets package. Operations include selecting and filtering columns, sorting rows, creating new variables, and statistically summarizing the data. The complexity of this code is rated 3/10, as it uses basic, common dplyr functions that are essential for data analysis in R.","Full Code Description

    Load Libraries and Data:
    The necessary libraries (dplyr and datasets) are loaded and the iris data set is printed.

    Exercise 1: Selecting Specific Columns
    The Sepal.Length, Sepal.Width, and Petal.Length columns are selected from the iris data set.

    Exercise 2: Selecting All Columns Except One
    All columns in the iris data set except Petal.Width are selected.

    Exercise 3: Selecting Columns Starting with a Specific Letter
    Columns whose names begin with the letter ""P"" are selected.

    Exercise 4: Filtering Rows with Specific Conditions
    Rows where Sepal.Length is greater than or equal to 4.6 and Petal.Width is greater than or equal to 0.5 are filtered out.

    Exercise 5: Rearrangement of Columns
    The Sepal.Width and Sepal.Length columns are selected and reordered.

    Exercise 6: Sorting Rows by a Column
    The rows of the iris data set are sorted by the Sepal.Width values.

    Exercise 7: Selection and Double Sorting of Columns
    The first three columns of the iris data set are selected and saved to irisAux. The irisAux rows are then sorted first by Sepal.Length and then by Sepal.Width.

    Exercise 8: Creating a New Column
    A new column proportion is added which is the proportion of Sepal.Length over Sepal.Width.

    Exercise 9: Statistical Summary of a Column
    Sepal.Length is averaged and stored in avg_slength.

    Exercise 10: Attempted Inappropriate Summary Operation
    An attempt is made to split the entire iris data set by Sepal.Length, which is not a standard dplyr operation and will cause an error.",https://github.com/AndresWV/procesamiento-irisDataset-r,R,3
icoProjects.jpg,Exploratory Data Analysis with Pandas in a Jupyter Notebook,"This repository contains a Jupyter Notebook designed to perform exploratory data analysis using the pandas library in Python. The notebook covers everything from loading and previewing data to cleaning, transforming and viewing it. The code complexity is 4/10, making it suitable for beginners with basic knowledge of Python and pandas.","The notebook starts by importing the necessary libraries (pandas and numpy). Next, a CSV data set is loaded, previewed and examined using functions such as head(), info(), and describe(). Data cleansing operations are performed, including null value handling and data type conversion. Exploratory analysis includes creating new columns, filtering data, and aggregating using groupby(). Finally, basic visualizations are generated using matplotlib and seaborn to better interpret the data.",https://github.com/AndresWV/intro-series-dataframes-pandas,"PYTHON, PANDAS, NUMPY",4
icoProjects.jpg,Injector and extractor of BPMN to XMI,"The ""injector and extractor"" project is a Java-based backend that handles BPMN (Business Process Model and Notation) files, converting them into XMI (XML Metadata Interchange) format. An injector reads and processes BPMN files, while an extractor transforms these files into XMI, facilitating standardized representation and interoperability between different modeling tools and platforms. The main goal is to enable efficient data exchange and integration for further analysis and storage.","
The ""injector and extractor"" project is a Java-based backend focused on data injection and extraction, particularly for handling BPMN (Business Process Model and Notation) files and converting them into XMI (XML Metadata Interchange) format. An injector reads and processes BPMN files, while an extractor converts these files into XMI format, which is widely used for exchanging metadata information via XML.

The primary functionality is to facilitate the transformation of business process models (BPMN) into a format (XMI) that can be easily used for further analysis, storage, or integration with other systems. Converting BPMN to XMI allows for standardized representation and interoperability between different modeling tools and platforms.",https://github.com/AndresWV/injectorExtractorBack/tree/main,"JAVA, BPMN, XMI",
icoProjects.jpg,Apache Beam Environment with Airflow,"The project sets up Apache Beam locally with Airflow, covering virtual environment creation, Airflow configuration, database setup, and DAG creation for task automation with example Python functions.","The ""Apache Beam Environment with Airflow"" repository offers a local setup for running Apache Beam without Docker Compose. The project includes detailed instructions for setting up a Python virtual environment and configuring Apache Airflow. Users are guided through exporting environment variables, installing Airflow, initializing the database, and creating user roles. It explains how to start the Airflow web server and scheduler in separate terminals. Additionally, the repository demonstrates creating a DAG folder, writing Python functions for tasks like scraping, processing, and saving data, and defining task dependencies within a DAG.",https://github.com/AndresWV/venv-apache-beam-hw,"APACHE BEAM, AIRFLOW, VENV",